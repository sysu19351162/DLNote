# 李宏毅人工智能笔记

## 1-Regression

### Example

​		stock market forecast

​		self-driving car

​		Recommendation

### Example Application

​		Pokemon

#### Step 1——Model

​		先假设一个函数	y = b + w*x	w and b are parameters

​																(can be any value)

​		input a set of function to this linear(线性的) model

#### Step 2——Goodness of function

​		定义一个loss function

​		Input: a function

​		output: how bad it is

![img](https://img-blog.csdnimg.cn/2018122009544494.png)

​		

​			![img](https://img-blog.csdnimg.cn/20181220095911715.png)

​			估测误差

#### Step 3——Best function

​		运用Gradient Descent计算

​		条件：可微分

​		注意：求出的是local optimal,而不是global optimal

​		但是在linear regression里找到的一定是global optimal

​		补充解释：

​		~梯度的本意是一个向量（矢量），表示某一函数在该点处的方		向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此		梯度的方向）变化最快，变化率最大（为该梯度的模）。

​		~梯度下降小结：https://www.cnblogs.com/pinard/p/5970503.html



​		求出了之后我们用原数据集来检测的到一个average error,发现		当我们考虑二次项、三次项$ \cdots$ ,会发现average error逐渐降低。

​		但如果我们再次收集一个数据集，  放入这个模型检测会发现	       		average error呈现一个先降后增的趋势，这个后来的增加我们称  		做模型的过拟合（overfitting）。

​		在本次的例子中老师选择再获得60只宝可梦，将他们的数值画到		图表上，发现数值并不是一次，二次或者三次。。。的关系，其		中还有其他因素。

##### 	回到step 1

​		这个时候我们再回到Step 1重新设计model并将能想到的因素都		加入进函数中且仍按照之前的方法写成线性的。

​		但这样函数就会很复杂，会出现过拟合。但我们又不知道哪些因		素是没有用的，没法删去，为了解决这个问题我们就采用正则化	（Regularization）。

​		Regularization要做的事情就是，我们重新定义了一个function		是好还是坏，即重新定义了Loss function。我们在原来的Loss 		function的基础上加上了一些knowledge，让我们可以得到比较		好的参数。

###### 正则化的具体做法

​	假如我们用的还是原来的一次函数，原来的loss function只考虑了	error,那么正则化就是在后面再加上一项。其中的$\lambda$是需要	我们手调的一项参数。![img](https://img-blog.csdnimg.cn/20181220180207713.png)

​	加入的这一项是表示$w_i$越小越好,$w_i$小意味着我们会得到一个平滑	的function，而平滑的funcion对输入比较不敏感，收到噪声的干扰	会更小，从而给我们一个更好的结果

###### $\lambda$的意义

​	$\lambda$越大意味着我们更多的考虑$w_i$，减少考虑error

​	这是实验结果

![img](https://img-blog.csdnimg.cn/20181220193802423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BlbmdjaGVuZ2xpdQ==,size_16,color_FFFFFF,t_70)

​	我们看一下training data上的error，发现越平滑，error越大。这	件事是非常合理的，因为当λ越大时，我们就偏向于考虑wi本身的	值，减少考虑error。所以当λ越大，考虑error越少。所以在		   training data 上得到的error越大。

​	但是有趣的是，虽然training data上得到的error越大，但是在	testing data上得到的error可能会比较小。比如当λ=100时，error为11.1。

​	这个结果也是合理的，因为我们喜欢比较平滑的function，比较平	滑的function对噪声比较不敏感，所以当增加λ时，performance	越来越好。但是我们又不喜欢太平滑的function，因为最平滑的		function就是一条水平线，如果是水平线，那什么也干不成了。	所以如果function太平滑的话，反而会得到很糟糕的结果。所以，	现在的问题是看我们的model多平滑，这件事就必须通过调λ来解	决。比如这里λ为100时，得到比较好的model。
